* updated the tokenizer to whitespace tokenizer to avoid date tokenization as different words
* updated tokenizer, now using trained tokenizer for legacy.